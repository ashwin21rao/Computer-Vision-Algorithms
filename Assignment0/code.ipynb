{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55147691",
   "metadata": {},
   "source": [
    "# Task 1: Converting video to images and vice versa\n",
    "\n",
    "A video is basically a sequence of images (frames). This question has 2 parts:\n",
    "\n",
    "- Splitting a video into its corresponding frames and saving each frame as a separate image.\n",
    "- Combining a sequence of images into a video.\n",
    "\n",
    "Through this question, I learned:\n",
    "\n",
    "- How to read videos using OpenCV VideoCapture.\n",
    "- How to write videos using OpenCV VideoWriter given an image sequence.\n",
    "- How to access properties of a video using OpenCV (width, height, FPS, etc).\n",
    "- How to access each frame of a video in a loop and perform operations on them.\n",
    "- How to write an image to a file using OpenCV.\n",
    "- Working with file structures in python (creating and deleting files and directories).\n",
    "\n",
    "## Experiments\n",
    "\n",
    "- I experimented with different FPS rates for the video created by combining the image sequence generated in the first part of the question. The required FPS can be passed as a parameter to the function which creates the new video.\n",
    "    - If the FPS of the new video is the same as the FPS of the original video, then the new video has the same length as the original video.\n",
    "    - If the FPS of the new video is greater than the FPS of the original video, the resultant video is longer. Similarly, if the FPS of the new video is less than the FPS of the original video, the resultant video is shorter.\n",
    "- The video can be created by combining the image sequence in two ways:\n",
    "    - Manually looping over each image in the sequence. In this case, we have no metadata about the original video like its FPS. However, we can get the width and height of the video from its constituent frames.\n",
    "    - Creating a VideoCapture object from the image sequence and iterating over its frames, similar to the first part of this task. In this case, we have metadata about the original video like its FPS, width and height readily available from the VideoCapture object.\n",
    "- The VideoWriter API can create a video in different formats (AVI, MP4, etc). The codecs used for specific formats are operating system dependent. I experimented with different codecs and found that on my system, the codec `mp4v` worked to convert an mp4 into another mp4. Codecs `XVID` and `MJPG` did not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d386601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitVideoIntoFrames(video_path, save_path):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # empty contents of save_path or create it\n",
    "    if os.path.exists(save_path) and os.path.isdir(save_path):\n",
    "        shutil.rmtree(save_path)\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "    # process video frame by frame\n",
    "    frame_num = 1\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # write frame to file\n",
    "        cv2.imwrite(os.path.join(save_path, f\"frame{str(frame_num).zfill(6)}.png\"), frame)\n",
    "        frame_num += 1\n",
    "        \n",
    "        # display frame in jupyter notebook\n",
    "        fig = plt.figure(figsize=(16, 8))\n",
    "        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42063f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeFramesIntoVideo(img_path, video_path, fps=None):\n",
    "    filename = os.listdir(img_path)[0]\n",
    "\n",
    "    frame_num_len = None\n",
    "    for i, c in enumerate(filename):\n",
    "        if c.isdigit():\n",
    "            frame_num_len = len(filename[i: filename.find(\".\")])\n",
    "            break\n",
    "\n",
    "    video = cv2.VideoCapture(os.path.join(img_path, f\"frame%0{frame_num_len}d.png\"))\n",
    "    width = int(video.get(3))\n",
    "    height = int(video.get(4))\n",
    "    if fps is None:\n",
    "        fps = int(video.get(5))\n",
    "\n",
    "    out = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    # process each frame\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # write frame to video\n",
    "        out.write(frame)\n",
    "        \n",
    "        # display frame in jupyter notebook\n",
    "        fig = plt.figure(figsize=(16, 8))\n",
    "        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    video.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f332cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_path = \"q1/frames\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459f71c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "splitVideoIntoFrames(\"q1/race.mp4\", frames_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b91d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeFramesIntoVideo(frames_path, \"q1/merged.mp4\", fps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697a3c42",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Frames from the original video and those of the new video are shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ce2146",
   "metadata": {},
   "source": [
    "# Task 2: Capturing video from a webcam and displaying it\n",
    "\n",
    "This task builds upon the previous one. We can use the same VideoCapture class of OpenCV to capture videos from a webcam. Instead of specifying a path to the video (as in the previous task), we specify 0 to access the webcam.\n",
    "\n",
    "Through this question, I learned:\n",
    "\n",
    "- How to read a live webcam stream using OpenCV.\n",
    "- How to display images using OpenCV in an interactive window.\n",
    "\n",
    "### Experiments\n",
    "\n",
    "- Performing specific actions in the webcam stream and observing the generated image sequence to see if they match the original stream.\n",
    "- Finding FPS of the webcam. This data is available through the VideoCapture object. It is `30fps` for my webcam.\n",
    "- Comparing the number of frames saved and the FPS of the webcam to the length of the webcam stream. $Length\\ of\\ video\\ (s) = fps \\times number\\ of\\ frames$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def captureWebcam(save_path):\n",
    "    video = cv2.VideoCapture(0)\n",
    "\n",
    "    # fps of webcam\n",
    "    print(video.get(5))\n",
    "\n",
    "    # empty contents of save_path or create it\n",
    "    if os.path.exists(save_path) and os.path.isdir(save_path):\n",
    "        shutil.rmtree(save_path)\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "    # process video frame by frame\n",
    "    frame_num = 1\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # write frame to file\n",
    "        cv2.imwrite(os.path.join(save_path, f\"frame{str(frame_num).zfill(6)}.png\"), frame)\n",
    "        frame_num += 1\n",
    "\n",
    "        # display frame in window\n",
    "        cv2.imshow(\"webcam\", frame)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "        \n",
    "        # display frame in jupyter notebook\n",
    "        fig = plt.figure(figsize=(16, 8))\n",
    "        plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b96de2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frames_path = \"q2/frames\"\n",
    "captureWebcam(frames_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9430207e",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Frames from the captured webcam stream are shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac053ba",
   "metadata": {},
   "source": [
    "# Task 3: Chroma Keying\n",
    "\n",
    "This is popularly known as the **green screen effect**. It involves replacing a green screen (usually in the background) of a photo or video with a new background. In this task, I learned the concepts behind the green screen effect and how to apply it to both images and videos.\n",
    "\n",
    "Chroma keying is done by simple colour thresholding, that is, identifying all pixels in a certain range of colour and applying a mask to remove them. The specific process is as follows:\n",
    "\n",
    "- Find all pixels within a certain colour range (in this case, green pixels).\n",
    "- Create a mask to filter out all these pixels in the specified range.\n",
    "- Combine the 2 images using this mask as follows:\n",
    "    - Suppose we wish to apply a background `bg` to a frame `img` and the `mask` is 1 at all pixels belonging to the green screen in `img`.\n",
    "    - We combine them as `img[mask != 0] = bg[mask != 0]`.\n",
    "    - This could also be done using bitwise operators like `bitwise_and`.\n",
    "\n",
    "To apply this effect on 2 videos, we process each video frame by frame and perform chroma keying for each pair of frames.\n",
    "\n",
    "## Experiments\n",
    "\n",
    "- Finding a good range to properly filter out the green screen involved a significant amount of experimentation. It highly depends on the quality and consistency of the green screen. The video I chose with the green screen also had shadows on the green screen. This means that the colour threshold needed to cover a wider range of green to remove these darker shades as well. In the end, I chose the range [0, 80, 0] to [85, 255, 85].\n",
    "- After applying chroma keying, a thin green outline was visible on the foreground objects. This is because these pixels on the edges also had green mixed in them. To remove this outline, I tried the following:\n",
    "    - Detecting edges (using Canny edge detection) and making these pixels transparent (by setting alpha to 0). This can be done by applying the edges as a mask to the original image. Although this works for images, it did not work for videos since the VideoWriter class does not allow RGBA images as input (where A is the alpha channel).\n",
    "    - Changing the range of colours to be considered as a part of the green screen. After much experimentation, the chosen range seems to minimize the green outline without affecting the rest of the image. For example, an upper threshold of [80, 255, 80] resulted in the green outline being more prominent. An upper threshold of [90, 255, 90] filtered out more points but resulted in other parts of the images being removed as well, which was not desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d716bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeGreenScreen(video_path, background_path, output_path):\n",
    "    # foreground\n",
    "    video1 = cv2.VideoCapture(video_path)\n",
    "    width = int(video1.get(3))\n",
    "    height = int(video1.get(4))\n",
    "\n",
    "    # background\n",
    "    video2 = cv2.VideoCapture(background_path)\n",
    "\n",
    "    # combined video\n",
    "    fps = int(video1.get(5))\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    # process video frame by frame\n",
    "    while True:\n",
    "        ret1, frame1 = video1.read()\n",
    "\n",
    "        ret2, frame2 = video2.read()\n",
    "        frame2 = cv2.resize(frame2, (width, height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        if not ret1 or not ret2:\n",
    "            break\n",
    "\n",
    "        # generate mask\n",
    "        l_green = np.array([0, 80, 0])\n",
    "        u_green = np.array([85, 255, 85])\n",
    "        mask = cv2.inRange(frame1, l_green, u_green)\n",
    "\n",
    "        # apply mask\n",
    "        combined = np.copy(frame1)\n",
    "        combined[mask != 0] = frame2[mask != 0]\n",
    "        \n",
    "        # display frames in jupyter notebook\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\n",
    "        ax1.axis('off')\n",
    "        ax2.axis('off')\n",
    "        ax3.axis('off')\n",
    "        ax1.imshow(cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB))\n",
    "        ax2.imshow(cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB))\n",
    "        ax3.imshow(cv2.cvtColor(combined, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        \n",
    "        # write frame to video\n",
    "        out.write(combined)\n",
    "\n",
    "    video1.release()\n",
    "    video2.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a64e99f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "removeGreenScreen(\"q3/video.mp4\", \"q3/background_long.mp4\", \"q3/combined.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10ebaf1",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Frames from the original videos. Notice the shadows in the first frame which made choosing the colour threshold for chroma keying more difficult.\n",
    "\n",
    "![orig.png](q3/orig.png)\n",
    "\n",
    "![bg.png](q3/bg.png)\n",
    "\n",
    "Corresponding frame in the combined video after performing chroma keying. Notice that the green outline exists but does not completely surround the monkeys in the foreground.\n",
    "\n",
    "![new.png](q3/new.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcbabf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerVision",
   "language": "python",
   "name": "computervision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
